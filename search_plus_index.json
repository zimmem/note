{"./":{"url":"./","title":"Introduction","keywords":"","body":" 学习笔记 zimmem 的编程笔记 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/DEFAULT.html":{"url":"big-data-platform-ops/cdh/DEFAULT.html","title":"cdh","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/create-cloudera-manager-extensions.html":{"url":"big-data-platform-ops/cdh/create-cloudera-manager-extensions.html","title":"Create Cloudera Manager extensions ","keywords":"","body":"Create Cloudera Manager extensions 构建 parcel 包 parcel 描述文件 meta/parcel.json 文件列表 meta/filelist.json Defind Script meta/cos_env.json 软链接描述 meta/alternatives.json 构建 parcel 文件 安装 parcel （略） 修改 hdfs配置 success 以支持腾讯云 COS 为例 构建 parcel 包 通过 parcel 机制给 cdh 中的 yarn 添加 cos 支持， 使 yarn 应用能直接访问 cos 上的文件 腾讯云 cos 支持通过 hdfs 访问(Hadoop 工具)， 但公司大数据集群使用 CDH 管理， 直接修改配置虽然可行， 但每台机器都登录上去部署非常繁琐， 且万一下次通过 Clouder Manager 更新配置， 又会把配置覆盖掉， 所以要找出一种方式看能否通过 clouder manager 来做这种增强， 查了CDH 官方资料后发现应该可以通过 pracel 以插件的方式来把 cos-hadoop 部署到集群中。 parcel 文件格式参考The parcel format， 主要有以下几个文件组成 meta/parcel.json meta/filelist.json meta/cos_env.json meta/alternatives.json 其它 lib 文件 parcel 描述文件 meta/parcel.json 格式参考 The parcel.json file { \"schema_version\": 1, \"name\": \"HADOOP_COS\", \"version\": \"2.7.2.2\", \"extraVersionInfo\": { \"fullVersion\": \"2.7.2.2\", \"baseVersion\": \"2.7.2.2\", \"patchCount\": \"\" }, \"conflicts\": \"CDH (> 5.13.1.)\", \"setActiveSymlink\": true, \"scripts\": { \"defines\": \"cos_env.sh\" }, \"packages\": [ { \"name\": \"cos-hadoop\", \"version\": \"2.7.2.2\" } ], \"components\": [ { \"name\": \"cos-hadoop\", \"version\": \"2.7.2.2\", \"pkg_version\": \"2.7.2.2\" } ], \"provides\": [ \"yarn-plugin\", \"mapreduce-plugin\", \"mapreduce2-plugin\", \"spark-plugin\", \"hive-plugin\" ], \"users\": { }, \"groups\": [ ] } 文件列表 meta/filelist.json 描述插件中每个组件的文件列表 { \"cos-hadoop\" : { \"name\": \"cos-hadoop\", \"version\": \"2.7.2\", \"files\" : { \"jars\" : {}, \"jars/cos_api-4.2.jar\" : {}, \"jars/hadoop-cos-2.7.2.jar\" : {}, \"jars/httpmime-4.2.5.jar\" : {}, \"json-20140107.jar\" : {} } } } Defind Script meta/cos_env.json Defind Script 用于在插件影响到的组件启动时执行， 一般是把插件的 jar 包加入到组件的 classpath 中。 组件启动时以 source defind.script 方式调用。 插件影响到哪些组件在 meta/parcel.json 中的 provides 定义， 组件名称参考 Service parcel tags recognised by cloudera manager, 脚本中用到的 classpath 变量名参考 Plugin parcel environment variables #!/bin/bash HADOOP_COS_DIRNAME=${PARCEL_DIRNAME:-\"HADOOP_COS\"} # if [ -n \"${HADOOP_CLASSPATH}\" ]; then # export HADOOP_CLASSPATH=\"${HADOOP_CLASSPATH}:$PARCELS_ROOT/$HADOOP_COS_DIRNAME/jars/*\" # else # export HADOOP_CLASSPATH=\"$PARCELS_ROOT/$HADOOP_COS_DIRNAME/jars/*\" # fi if [ -n \"${MR2_CLASSPATH}\" ]; then export MR2_CLASSPATH=\"${MR2_CLASSPATH}:$PARCELS_ROOT/$HADOOP_COS_DIRNAME/jars/*\" else export MR2_CLASSPATH=\"$PARCELS_ROOT/$HADOOP_COS_DIRNAME/jars/*\" fi 软链接描述 meta/alternatives.json 用于建立软链接， hadoop_cos 中没有用到， 参考The alternatives.json file 构建 parcel 文件 参考 Building a parcel, parcel 文件实际是一个 .tar.gz 文件 tar zcvf GPLEXTRAS-5.0.0-gplextras5b2.p0.32-el6.parcel GPLEXTRAS-5.0.0-gplextras5b2.p0.32/ --owner=root --group=root parcel 文件名必须符合格式 ${parcel_name}-${version}-${distro_suffixes}.pracel, 其中 ${distro_suffixes} 参考 Parcel distro suffixes 使用 validator 检验文件合法性 使用 make_manifest.py 创建 parcel 描述 文件 安装 parcel （略） 修改 hdfs配置 路径 Cloudera Manager -> HDFS -> Configuration -> Scope(Gateway) 配置项 HDFS Client Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml 内容 fs.cos.userinfo.appid your cos app id fs.cos.userinfo.secretId your cos secret id fs.cos.userinfo.secretKey your cos secret key fs.cosn.impl org.apache.hadoop.fs.cosnative.NativeCosFileSystem fs.cos.buffer.dir /data/cos/buffer fs.cos.userinfo.region gz var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/create-custom-cloudera-add-on-service.html":{"url":"big-data-platform-ops/cdh/create-custom-cloudera-add-on-service.html","title":"Create Custom Cloudera Add-on Service","keywords":"","body":"Create Custom Cloudera Add-on Service var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/create-custom-repository.html":{"url":"big-data-platform-ops/cdh/create-custom-repository.html","title":"Create Custom Repository ","keywords":"","body":"Create Custom Repository create cloudera manager yum repository sudo yum install yum-utils createrepo wget https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo -O /etc/yum.repo.d/cloudera-manager.repo reposync -r cloudera-manager cd cloudera-manager createrepo . ## upload to http server create custom parcels # download all file on https://archive.cloudera.com/cdh5/parcels/5.13.1.2/ # upload to http server [1]: https://www.cloudera.com/documentation/enterprise/5-13-x/topics/cdh_ig_yumrepo_local_create.html [2]:https://archive.cloudera.com/cdh5/parcels/5.13.1.2/ var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/extjs-dependency-by-oozie.html":{"url":"big-data-platform-ops/cdh/extjs-dependency-by-oozie.html","title":"extjs-dependency-by-oozie.md","keywords":"","body":"http://archive.cloudera.com/gplextras/misc/ext-2.2.zip var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/install-cdh-on-ubuntu-16.html":{"url":"big-data-platform-ops/cdh/install-cdh-on-ubuntu-16.html","title":"install-cdh-on-ubuntu-16.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/install-cloudera-manager-on-centos-6.html":{"url":"big-data-platform-ops/cdh/install-cloudera-manager-on-centos-6.html","title":"Install Cloudera Manager on CentOS 6","keywords":"","body":"Install Cloudera Manager on CentOS 6 # add repository or use your local repository wget https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo -O /etc/yum.repos.d/cloudera-manager.repo # install JDK or you may install offical 1.8 jdk Manually from oracle yum install oracle-j2sdk1.7 # install cloudera-manager-server and cloudera-manager-deamon yum install cloudera-manager-daemons cloudera-manager-server # download mysql-connector-java from http://www.mysql.com/downloads/connector/j/5.1.html # make sure the mysql-connector-java place at /usr/share/java/mysql-connector-java.jar # prepare database /usr/share/cmf/schema/scm_prepare_database.sh -h ${mysql-host} mysql ${schema} ${user} # start cloudera-scm-server service cloudera-scm-server start [1]:https://www.cloudera.com/documentation/enterprise/release-notes/topics/rn\\_consolidated\\_pcm.html [2]:https://www.cloudera.com/documentation/enterprise/5-13-x/topics/cm\\_ig\\_mysql.html var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/install-cm-on-ubuntu-16.html":{"url":"big-data-platform-ops/cdh/install-cm-on-ubuntu-16.html","title":"Install Cloudera Manager on Ubuntu 16","keywords":"","body":"Install Cloudera Manager on Ubuntu 16 Install JDK cd /etc/apt/source.list.d/ # add cloudera source wget https://archive.cloudera.com/cm5/ubuntu/xenial/amd64/cm/cloudera.list # add gpg key curl https://archive.cloudera.com/cm5/ubuntu/xenial/amd64/cm/archive.key | apt-key add - apt update # install jdk 1.7 apt-get install oracle-j2sdk1.7 #install cm apt-get install cloudera-manager-daemons cloudera-manager-server var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/cdh/readwrite-hive-table-when-running-spark2-by-oozie.html":{"url":"big-data-platform-ops/cdh/readwrite-hive-table-when-running-spark2-by-oozie.html","title":"Read/Write hive table when running spark2 by oozie","keywords":"","body":"Read/Write hive table when running spark2 by oozie 问题描述 CDH 5.12 集群安装 spark2 后, 在机器上直接通过 spark2-shell 或 spark2-submit 都能支持 spark.sql 读写 hive 表， 但当通过 oozie 的 shell action 时， spark 读取不到 hive catalog ， 因而无法读写 hive 表， 不管是直接在 shell 结点直接调 spark2-submit 或者把命令放在 sh 脚本里， 也不管 --deploy-mode 是 client 或 cluster 问题查找 通过对比两种不同方式调起的 java 进程的 classpath , 发现通过 oozie 调起的进程 classpath 中少了 /opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/conf/yarn-conf 这个目录 使用 debug 模式(bash -x) 诊断上述目录是如何加入到 classpath 中的， 最终发现在 /opt/cloudera/parcels/SPARK2-2.2.0.cloudera2-1.cdh5.12.0.p0.232957/lib/spark2/conf/spark-env.sh 中有HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-$SPARK_CONF_DIR/yarn-conf} 用同样方法查找 oozie 中执行 spark2-submit 没有把上述目录加入 classpath 的原因， 发现oozie 执行脚本是， HADOOP_CONF_DIR 已被设置为 /data/yarn/nm/usercache/icarbonx/appcache/application_1521461396754_4654/container_e37_1521461396754_4654_01_000002/oozie-hadoop-conf-1523771262850， 继续阅读 oozie 相关源码，找到最终原因， 相关代码如下 //https://github.com/apache/oozie/blob/release-4.3.1/sharelib/oozie/src/main/java/org/apache/oozie/action/hadoop/ShellMain.java public class ShellMain extends LauncherMain { private void prepareHadoopConfigs(Configuration actionConf, Map envp, File currDir) throws IOException { if (actionConf.getBoolean(CONF_OOZIE_SHELL_SETUP_HADOOP_CONF_DIR, false)) { String actionXml = envp.get(OOZIE_ACTION_CONF_XML); if (actionXml != null) { File confDir = new File(currDir, \"oozie-hadoop-conf-\" + System.currentTimeMillis()); writeHadoopConfig(actionXml, confDir); if (actionConf.getBoolean(CONF_OOZIE_SHELL_SETUP_HADOOP_CONF_DIR_WRITE_LOG4J_PROPERTIES, true)) { System.out.println(\"Writing \" + LOG4J_PROPERTIES + \" to \" + confDir); writeLoggerProperties(actionConf, confDir); } System.out.println(\"Setting \" + HADOOP_CONF_DIR + \" and \" + YARN_CONF_DIR + \" to \" + confDir.getAbsolutePath()); envp.put(HADOOP_CONF_DIR, confDir.getAbsolutePath()); envp.put(YARN_CONF_DIR, confDir.getAbsolutePath()); } } } } //https://github.com/apache/oozie/blob/release-4.3.1/sharelib/oozie/src/main/java/org/apache/oozie/action/hadoop/LauncherMain.java public abstract class LauncherMain { protected static String[] HADOOP_SITE_FILES = new String[] {\"core-site.xml\", \"hdfs-site.xml\", \"mapred-site.xml\", \"yarn-site.xml\"}; protected void writeHadoopConfig(String actionXml, File basrDir) throws IOException { File actionXmlFile = new File(actionXml); System.out.println(\"Copying \" + actionXml + \" to \" + basrDir + \"/\" + Arrays.toString(HADOOP_SITE_FILES)); basrDir.mkdirs(); File[] dstFiles = new File[HADOOP_SITE_FILES.length]; for (int i = 0; i 解决方法 CDH -> Spark2 -> Configuration -> Spark 2 Client Advanced Configuration Snippet (Safety Valve) for spark2-conf/spark-env.sh 添加 if [ -n \"$SPARK_CONF_DIR/yarn-conf\" ] && [ \"$HADOOP_CONF_DIR\" != \"$SPARK_CONF_DIR/yarn-conf\" ]; then export SPARK_DIST_CLASSPATH=\"$SPARK_DIST_CLASSPATH:$SPARK_CONF_DIR/yarn-conf\" fi var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/hadoop/hdfs.html":{"url":"big-data-platform-ops/hadoop/hdfs.html","title":"hdfs.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/hadoop/resource-configuration.html":{"url":"big-data-platform-ops/hadoop/resource-configuration.html","title":"Resource Configuration","keywords":"","body":"Resource Configuration yarn.nodemanager.resource.memory-mb 配置每台机器可以提供的内存 yarn.scheduler.maximum-allocation-mb 配置每个 excutor 可以申请的最大内存 Spark源码分析之九：内存管理模型 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/hbase/":{"url":"big-data-platform-ops/hbase/","title":"README.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/hive/":{"url":"big-data-platform-ops/hive/","title":"README.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/impala/maintenance-instructions.html":{"url":"big-data-platform-ops/impala/maintenance-instructions.html","title":"Impala Maintenance Instructions","keywords":"","body":"Impala Maintenance Instructions refresh [schema].table 当 hdfs 文件发生变更后使用该使令刷新 INVALIDATE METADATA [[db_name.]table_name] 当 hive 表元数据发生变更，且不是由 Impala 引起的， 调用该指令刷新 metadata 缓存 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/spark/cvc.html":{"url":"big-data-platform-ops/spark/cvc.html","title":"cvc.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/spark/spark-dataframe-join.html":{"url":"big-data-platform-ops/spark/spark-dataframe-join.html","title":"Spark DataFrame Join","keywords":"","body":"Spark DataFrame Join Prepare Data case class Person(id:Int, name:String) case class Info(id:Int, age:Int) val personDF = sc.makeRDD(Seq(Person(1,\"A\"), Person(2, \"B\"))).toDF(\"id\", \"name\") val infoDF = sc.makeRDD(Seq(Info(1,11), Info(3, 33))).toDF(\"id\", \"age\") samples just join personDF.join(infoDF).show() +---+----+---+---+ | id|name| id|age| +---+----+---+---+ | 1| A| 1| 11| | 1| A| 3| 33| | 2| B| 1| 11| | 2| B| 3| 33| +---+----+---+---+ inner join using column personDF.join(infoDF, \"id\").show() +---+----+---+ | id|name|age| +---+----+---+ | 1| A| 11| +---+----+---+ left outer join personDF.join(infoDF, Seq(\"id\"), \"left_outer\" ).show() +---+----+----+ | id|name| age| +---+----+----+ | 1| A| 11| | 2| B|null| +---+----+----+ outer join personDF.join(infoDF, Seq(\"id\"), \"outer\" ).show() +---+----+----+ | id|name| age| +---+----+----+ | 1| A| 11| | 2| B|null| | 3|null| 33| +---+----+----+ right outer join personDF.join(infoDF, Seq(\"id\"), \"right_outer\" ).show() +---+----+---+ | id|name|age| +---+----+---+ | 1| A| 11| | 3|null| 33| +---+----+---+ var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/spark/spark-shell.html":{"url":"big-data-platform-ops/spark/spark-shell.html","title":"Spark Shell","keywords":"","body":"Spark Shell 提高 spark-shell 性能 spark-shell \\ --conf spark.driver.cores=2 \\ --driver-memory 3G \\ --num-executors 6 \\ --conf spark.executor.cores=3 \\ --executor-memory 4G var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/spark/spark-submit.html":{"url":"big-data-platform-ops/spark/spark-submit.html","title":"Spark Submit","keywords":"","body":"Spark Submit --conf \"spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps\" spark2-submit --class ClassName --master yarn --deploy-mode client --conf spark.driver.maxResultSize=0 --conf \"spark.executor.extraJavaOptions=-XX:+UseG1GC \" --conf spark.driver.cores=1 --conf spark.executor.cores=7 --conf spark.yarn.executor.memoryOverhead=3072 --driver-memory 3G --num-executors 6 --executor-memory 10G path-to.jar 25 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/apache-ambari-installation.html":{"url":"big-data-platform-ops/apache-ambari-installation.html","title":"Apache Ambari Installation","keywords":"","body":"Apache Ambari Installation # install mysql conector yum install mysql-connector-java [1]:https://docs.hortonworks.com/HDPDocuments/Ambari-2.5.2.0/bk_ambari-installation/content/download_the_ambari_repo_ubuntu16.html var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/big-data-platform-ops.html":{"url":"big-data-platform-ops/big-data-platform-ops.html","title":"big-data-platform-ops.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"big-data-platform-ops/hdp.html":{"url":"big-data-platform-ops/hdp.html","title":"hdp.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"blockchain/hyperledger-fabric/block-struct.html":{"url":"blockchain/hyperledger-fabric/block-struct.html","title":"Block Struct","keywords":"","body":"Block Struct Hyperledger Fabric V1.0: Block Structure (Part 1) Under Construction: Hyperledger Fabric V1.0: Block Structure (Part 2) var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"blockchain/hyperledger-fabric/concept.html":{"url":"blockchain/hyperledger-fabric/concept.html","title":"概念","keywords":"","body":"概念 # var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"blockchain/hyperledger-fabric/deploy-fabric-on-kubernetes.html":{"url":"blockchain/hyperledger-fabric/deploy-fabric-on-kubernetes.html","title":"在 kubernetes 上搭建 hyperledger-fabric","keywords":"","body":"在 kubernetes 上搭建 hyperledger-fabric 修改 K8S 结点 DNS 配置 两种方法， 修改 docker 启动参数 修改结点 /etc/resolv.conf 创建 namespace declare -a arr=(\"fabric-orderer\" \"fabric-org-1\" \"fabric-org-2\") for i in \"${arr[@]}\" do kubectl create namespace $i done 创建共享 Volumn 为方便 pod 间共享文件， 使用 nfs-volumn # read line with space IFS='' declare -a arr=(\"default\" \"fabric-orderer\" \"fabric-org-1\" \"fabric-org-2\") nfs_server=10.100.240.76 for ns in \"${arr[@]}\" do echo \" apiVersion: v1 kind: PersistentVolume metadata: name: nfs-fabric-dev-for-${ns} spec: accessModes: - ReadWriteMany capacity: storage: 10Gi nfs: path: / server: ${nfs_server} --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fabric-pvc-for-${ns} namespace: ${ns} spec: accessModes: - ReadWriteMany resources: requests: storage: 3Gi storageClassName: \"\" volumeName: nfs-fabric-dev-for-${ns} \" | kubectl create -f - done 搭建 fabric-tools apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: name: fabric-tools name: fabric-tools namespace: default spec: replicas: 1 selector: matchLabels: name: fabric-tools template: metadata: labels: name: fabric-tools spec: containers: - args: - sleep - \"999999999\" image: registry.docker-cn.com/hyperledger/fabric-tools:x86_64-1.1.0 name: fabric-tools volumeMounts: - mountPath: /etc/hyperledger/fabric name: fabric-config volumes: - name: fabric-config persistentVolumeClaim: claimName: fabric-pvc-for-default 生成配置文件 一个 orderer 结点 ， 两个组织， 每个组织两个 peer 进入 fabric-tool pod 的 bash kubectl exec -it fabric-tools-xxx bash 创建 crypto-config.yaml 并生成配置文件 cat > /etc/hyperledger/fabric/crypto-config.yaml 创建 /etc/hyperledger/fabric/configtx.yaml 文件 参考 https://raw.githubusercontent.com/hyperledger/fabric/v1.1/examples/e2e_cli/configtx.yaml Profiles: TwoOrgsOrdererGenesis: Orderer: 生成创世区块 cd /etc/hyperledger/fabric configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./crypto-config/ordererOrganizations/fabric-orderer/orderers/orderer.fabric-orderer/orderer.genesis.block CHANNEL_NAME=test-channel # 生成 channel configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./${CHANNEL_NAME}.tx -channelID ${CHANNEL_NAME} # 生成 org1, org2 的锚点文件 configtxgen \\ -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./Org1MSPanchors.tx \\ -channelID ${CHANNEL_NAME} \\ -asOrg Org1MSP configtxgen \\ -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./Org2MSPanchors.tx \\ -channelID ${CHANNEL_NAME} \\ -asOrg Org2MSP 搭建 orderer 结点 把 configtx.yaml 复制到 /etc/hyperledger/fabric/crypto-config/ordererOrganizations/fabric-orderer/orderers/orderer.fabric-orderer/configtx.yaml 创建 /etc/hyperledger/fabric/crypto-config/ordererOrganizations/fabric-orderer/orderers/orderer.fabric-orderer/orderer.yaml 参考 https://github.com/hyperledger/fabric/blob/v1.1.0/sampleconfig/orderer.yaml General: LedgerType: file ListenAddress: 127.0.0.1 ListenPort: 7050 TLS: Enabled: false PrivateKey: tls/server.key Certificate: tls/server.crt RootCAs: - tls/ca.crt ClientAuthRequired: false ClientRootCAs: Keepalive: ServerMinInterval: 60s ServerInterval: 7200s ServerTimeout: 20s LogLevel: info LogFormat: '%{color}%{time:2006-01-02 15:04:05.000 MST} [%{module}] %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}' GenesisMethod: provisional GenesisProfile: TwoOrgsOrdererGenesis GenesisFile: orderer.genesis.block LocalMSPDir: /etc/hyperledger/fabric/msp LocalMSPID: OrdererMSP Profile: Enabled: false Address: 0.0.0.0:6060 BCCSP: Default: SW SW: Hash: SHA2 Security: 256 FileKeyStore: KeyStore: Authentication: TimeWindow: 15m FileLedger: Location: /var/hyperledger/production/orderer Prefix: hyperledger-fabric-ordererledger RAMLedger: HistorySize: 1000 Kafka: Retry: ShortInterval: 5s ShortTotal: 10m LongInterval: 5m LongTotal: 12h NetworkTimeouts: DialTimeout: 10s ReadTimeout: 10s WriteTimeout: 10s Metadata: RetryBackoff: 250ms RetryMax: 3 Producer: RetryBackoff: 100ms RetryMax: 3 Consumer: RetryBackoff: 2s Verbose: false TLS: Enabled: false PrivateKey: Certificate: RootCAs: Version: Debug: BroadcastTraceDir: DeliverTraceDir: 在 k8s 上创建 orderer 的 service 及 deployment apiVersion: v1 kind: Service metadata: name: orderer namespace: fabric-orderer spec: ports: - name: tcp-7050 port: 7050 protocol: TCP targetPort: 7050 selector: name: fabric-orderer --- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: name: fabric-orderer name: fabric-orderer namespace: fabric-orderer spec: replicas: 1 selector: matchLabels: name: fabric-orderer template: metadata: labels: name: fabric-orderer spec: containers: - args: - orderer - start image: registry.docker-cn.com/hyperledger/fabric-orderer:x86_64-1.1.0 name: fabric-orderer env: - name: ORDERER_GENERAL_LISTENADDRESS value: 0.0.0.0 - name: ORDERER_GENERAL_TLS_ENABLED value: \"true\" - name: ORDERER_GENERAL_TLS_ROOTCAS value: \"[/etc/hyperledger/fabric/tls/ca.crt]\" - name: ORDERER_GENERAL_LOCALMSPID value: OrdererMSP - name: ORDERER_GENERAL_LOCALMSPDIR value: /etc/hyperledger/fabric/msp volumeMounts: - mountPath: /etc/hyperledger/fabric/ name: fabric-config subPath: crypto-config/ordererOrganizations/fabric-orderer/orderers/orderer.fabric-orderer - mountPath: /etc/hyperledger/fabric/crypto-config name: fabric-config subPath: crypto-config volumes: - name: fabric-config persistentVolumeClaim: claimName: fabric-pvc-for-fabric-orderer 搭建 peer 结点 IFS='' declare -a orgIds=(\"1\" \"2\") for org_id in \"${orgIds[@]}\" do namespace=fabric-org-${org_id} declare -a peers=(\"peer0\" \"peer1\") for peer in \"${peers[@]}\" do echo \"apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: $namespace name: fabric-$peer spec: replicas: 1 template: metadata: labels: name: fabric-$peer spec: containers: - name: couchdb image: registry.docker-cn.com/hyperledger/fabric-couchdb:x86_64-1.0.0 ports: - containerPort: 5984 - name: peer image: registry.docker-cn.com/hyperledger/fabric-peer:x86_64-1.1.0 env: - name: CORE_LEDGER_STATE_STATEDATABASE value: \\\"CouchDB\\\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS value: \\\"localhost:5984\\\" - name: CORE_VM_ENDPOINT value: \\\"unix:///host/var/run/docker.sock\\\" - name: CORE_LOGGING_LEVEL value: \\\"DEBUG\\\" - name: CORE_PEER_TLS_ENABLED value: \\\"false\\\" - name: CORE_PEER_GOSSIP_USELEADERELECTION value: \\\"true\\\" - name: CORE_PEER_GOSSIP_ORGLEADER value: \\\"false\\\" - name: CORE_PEER_PROFILE_ENABLED value: \\\"true\\\" - name: CORE_PEER_TLS_CERT_FILE value: \\\"/etc/hyperledger/fabric/tls/server.crt\\\" - name: CORE_PEER_TLS_KEY_FILE value: \\\"/etc/hyperledger/fabric/tls/server.key\\\" - name: CORE_PEER_TLS_ROOTCERT_FILE value: \\\"/etc/hyperledger/fabric/tls/ca.crt\\\" - name: CORE_PEER_ID value: $peer.$namespace - name: CORE_PEER_ADDRESS value: 0.0.0.0:7051 - name: CORE_PEER_GOSSIP_EXTERNALENDPOINT value: $peer.$namespace:7051 - name: CORE_PEER_LOCALMSPID value: Org${org_id}MSP ports: - containerPort: 7051 - containerPort: 7052 - containerPort: 7053 command: [\\\"peer\\\"] args: [\\\"node\\\",\\\"start\\\"] volumeMounts: - mountPath: /etc/hyperledger/fabric/msp name: certificate subPath: crypto-config/peerOrganizations/$namespace/peers/$peer.$namespace/msp - mountPath: /etc/hyperledger/fabric/tls name: certificate subPath: crypto-config/peerOrganizations/$namespace/peers/$peer.$namespace/tls - mountPath: /host/var/run/ name: run volumes: - name: certificate persistentVolumeClaim: claimName: fabric-pvc-for-$namespace - name: run hostPath: path: /run --- apiVersion: v1 kind: Service metadata: namespace: $namespace name: $peer spec: selector: name: fabric-$peer ports: - name: externale-listen-endpoint protocol: TCP port: 7051 targetPort: 7051 - name: chaincode-listen protocol: TCP port: 7052 targetPort: 7052 --- \" | kubectl create -f - done done 运行示例 chaincode 创建 test-channel, 并把所有 peer 加入 进入 fabric-tools pod 执行 CHANNEL_NAME=test-channel export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/crypto-config/peerOrganizations/fabric-org-1/users/Admin@fabric-org-1/msp peer channel create \\ -o orderer.fabric-orderer:7050 \\ -c ${CHANNEL_NAME} \\ -f test-channel.tx # 创建成功本地生成 test-channel.block # 把4个结点都加入 channel declare -a orgIds=(\"1\" \"2\") for org_id in \"${orgIds[@]}\" do orgDomain=fabric-org-${org_id} declare -a peers=(\"peer0\" \"peer1\") for peer in \"${peers[@]}\" do export CORE_PEER_LOCALMSPID=\"Org${org_id}MSP\" export CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/crypto-config/peerOrganizations/${orgDomain}/users/Admin@${orgDomain}/msp export CORE_PEER_ADDRESS=$peer.${orgDomain}:7051 peer channel join -b test-channel.block done done # 更新两个组织的锚点信息 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/crypto-config/peerOrganizations/fabric-org-1/users/Admin@fabric-org-1/msp export CORE_PEER_ADDRESS=peer0.fabric-org-1:7051 peer channel update -o orderer.fabric-orderer:7050 \\ -c test-channel \\ -f Org1MSPanchors.tx export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/crypto-config/peerOrganizations/fabric-org-2/users/Admin@fabric-org-2/msp export CORE_PEER_ADDRESS=peer0.fabric-org-2:7051 peer channel update -o orderer.fabric-orderer:7050 \\ -c test-channel \\ -f Org2MSPanchors.tx 测试网络 进入 fabric-tools pod 的控制台 kubectl exec -it fabric-tools-xxx bash export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/crypto-config/peerOrganizations/fabric-org-1/users/Admin@fabric-org-1/msp export CORE_PEER_ADDRESS=peer0.fabric-org-1:7051 #下载 hyperledger/fabric 源码 git clone https://github.com/hyperledger/fabric.git /opt/gopath/src/github.com/hyperledger/fabric # 安装测试链码 peer chaincode install \\ -n test-cc \\ -v 1.0 \\ -p github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example02 # 实例化链码 # a 初始余额为 100 # b 初始余额为 200 peer chaincode instantiate \\ -o orderer.fabric-orderer:7050 \\ -C test-channel-1 \\ -n test-cc \\ -v 1.0 \\ -c '{\"Args\":[\"init\",\"a\",\"100\",\"b\",\"200\"]}' \\ -P \"OR('Org1MSP.member','Org2MSP.member')\" # 执行链码 # 从 a 的余额里转 10 到 b peer chaincode invoke \\ -o orderer.fabric-orderer:7050 \\ -C test-channel-1 \\ -n test-cc \\ -c '{\"Args\":[\"invoke\",\"a\",\"b\",\"10\"]}' # 查询 a 的余额 peer chaincode query \\ -C test-channel-1 \\ -n test-cc \\ -c '{\"Args\":[\"query\",\"a\"]}' # 查询 b 的余额 peer chaincode query \\ -C test-channel-1 \\ -n test-cc \\ -c '{\"Args\":[\"query\",\"b\"]}' var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/datasets/face.html":{"url":"data-process/datasets/face.html","title":"人脸数据集","keywords":"","body":"人脸数据集 Large-scale CelebFaces Attributes (CelebA) Dataset var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/dimension-reduction/DEFAULT.html":{"url":"data-process/dimension-reduction/DEFAULT.html","title":"降维","keywords":"","body":"降维 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/dimension-reduction/LDA.html":{"url":"data-process/dimension-reduction/LDA.html","title":"LDA.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/dimension-reduction/PCA.html":{"url":"data-process/dimension-reduction/PCA.html","title":"PCA 主成份分析","keywords":"","body":"PCA 主成份分析 主成分分析（PCA）原理详解 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/face/face-feature-point.html":{"url":"data-process/face/face-feature-point.html","title":"人脸关键点检测方法","keywords":"","body":"人脸关键点检测方法 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/machine-learning/gan/atricles.html":{"url":"data-process/machine-learning/gan/atricles.html","title":"GAN 学习资料","keywords":"","body":"GAN 学习资料 十款神奇的GAN，总有一个适合你！ var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/machine-learning/nlp/word2ver.html":{"url":"data-process/machine-learning/nlp/word2ver.html","title":"word2vec","keywords":"","body":"word2vec word2vec 中的数学原理详解 【不可思议的Word2Vec】6. Keras版的Word2Vec 【不可思议的Word2Vec】5. Tensorflow版的Word2Vec （二）通俗易懂理解——Skip-gram和CBOW算法原理 （三）通俗易懂理解——Skip-gram的负采样 Word2Vec笔记：Negative Sample var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/machine-learning/study-resource.html":{"url":"data-process/machine-learning/study-resource.html","title":"机器学习资源","keywords":"","body":"机器学习资源 google 机器学习速成课程 google Machine Learning Practica var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/DEFAULT.html":{"url":"data-process/math/DEFAULT.html","title":"数学知识","keywords":"","body":"数学知识 拉格朗日函数 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/statistics/DEFAULT.html":{"url":"data-process/math/statistics/DEFAULT.html","title":"统计学","keywords":"","body":"统计学 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/statistics/basic.html":{"url":"data-process/math/statistics/basic.html","title":"基础知识","keywords":"","body":"基础知识 方差 （Variance） 方差（Variance），应用数学里的专有名词。在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离。一个实随机变量的方差也称为它的二阶矩或二阶中心动差，恰巧也是它的二阶累积量。这里把复杂说白了，就是将各个误差将之平方（而非取绝对值，使之肯定为正数），相加之后再除以总数，透过这样的方式来算出各个数据分布、零散（相对中心点）的程度。继续延伸的话，方差的算术平方根称为该随机变量的标准差（此为相对各个数据点间）。 -- (https://zh.wikipedia.org/wiki/%E6%96%B9%E5%B7%AE) $\\sigma$ var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/statistics/corssentry.html":{"url":"data-process/math/statistics/corssentry.html","title":"交叉熵","keywords":"","body":"交叉熵 如何通俗的解释交叉熵与相对熵? - CyberRep的回答 - 知乎 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/statistics/variance-and-standard-deviation.html":{"url":"data-process/math/statistics/variance-and-standard-deviation.html","title":"# 方差与标准差","keywords":"","body":"方差与标准差 方差 （Variance） 方差（Variance），应用数学里的专有名词。在概率论和统计学中，一个随机变量的方差描述的是它的离散程度，也就是该变量离其期望值的距离。一个实随机变量的方差也称为它的二阶矩或二阶中心动差，恰巧也是它的二阶累积量。这里把复杂说白了，就是将各个误差将之平方（而非取绝对值，使之肯定为正数），相加之后再除以总数，透过这样的方式来算出各个数据分布、零散（相对中心点）的程度。继续延伸的话，方差的算术平方根称为该随机变量的标准差（此为相对各个数据点间）。 -- (https://zh.wikipedia.org/wiki/%E6%96%B9%E5%B7%AE) 基本定义 \r Var(X) =\\frac{1}{N} \\sum^N_{i=1}(x_i - \\mu)^2\r 标准差（Standard Deviation） 标准差（又称标准偏差、均方差，英语：Standard Deviation，缩写SD），数学符号σ（sigma），在概率统计中最常使用作为测量一组数值的离散程度之用。标准差定义：为方差开算术平方根，反映组内个体间的离散程度；标准差与期望值之比为标准离差率。 --（https://zh.wikipedia.org/wiki/%E6%A8%99%E6%BA%96%E5%B7%AE） 基本定义 \r SD=\\sqrt{ \\frac{1}{N} \\sum^N_{i=1}(x_i - \\mu)^2 }\r 协方差 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 --（https://zh.wikipedia.org/wiki/协方差） 期望值分别为$E(X)=\\mu$ 与 $E(Y)=\\nu$的两个具有有限二阶矩的实数随机变量X 与Y 之间的协方差定义为： \r cov(X,Y) = E((X-\\mu)(Y-v)) = E(X \\cdot Y )-\\mu v\r 协方差可以分析样本数据之间的线性相关性，协方差为正数时候，一般情况表示相关，协方差为负数的时候则表示不相关，常见的相关性计算就是基于协方差实现。在图像的直方图数据比较中是常规的手段之一。OpenCV与ImageJ中均有代码实现。 -- (https://blog.csdn.net/jia20003/article/details/72765439) var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/mathjax-demo.html":{"url":"data-process/math/mathjax-demo.html","title":"Mathjax Demo","keywords":"","body":"Mathjax Demo 基本语法 \r y=x_1+x^2_1 \r 行内公式 这是行内公式 a=b 矩阵 \r \\begin{matrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{matrix}\r \r \\begin{pmatrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{pmatrix}\r \r \\begin{bmatrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{bmatrix}\r \r \\begin{Bmatrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{Bmatrix}\r \r \\begin{vmatrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{vmatrix}\r \r \\begin{Vmatrix}\r \t1 & x & x^2 \\\\\r \t1 & y & y^2 \\\\\r \t1 & z & z^2 \\\\\r \\end{Vmatrix}\r \r \\begin {pmatrix}\r 1 & a_1 & a_1^2 & \\cdots & a_1^n \\\\\r 1 & a_2 & a_2^2 & \\cdots & a_2^n \\\\\r \\vdots & \\vdots& \\vdots & \\ddots & \\vdots \\\\\r 1 & a_m & a_m^2 & \\cdots & a_m^n \r \\end {pmatrix} \r \r \\left [\r \\begin{array} {cc|c}\r 1&2&3\\\\\r 4&5&6\r \\end {array}\r \\right ] \r 这是行内矩阵 a = \\bigl( \\begin{smallmatrix}1 & 1\\\\ 0 & 1\\end{smallmatrix}\\bigr) -10-8-6-4-202468101009080706050403020100 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/math/matrix.html":{"url":"data-process/math/matrix.html","title":"矩阵知识 {matrix}","keywords":"","body":"矩阵知识 {matrix} 矩阵类型 矩阵的逆 酉矩阵 正规矩阵 正交矩阵 矩阵操作 矩阵相加 矩阵相乘 \r a * \\begin{bmatrix}\r x_1, x_2 \\\\\r x_3, x_4 \\\\\r \\end{bmatrix} \r =\r \\begin{bmatrix}\r ax_1, ax_2 \\\\\r ax_3, ax_4 \\\\\r \\end{bmatrix}\r 矩阵倒置ma var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/recomand-system/atricles-and-systems.html":{"url":"data-process/recomand-system/atricles-and-systems.html","title":"推荐系统相关文章与开源方案","keywords":"","body":"推荐系统相关文章与开源方案 相关文章 深度解析京东个性化推荐系统演进史 可解释推荐系统：身怀绝技，一招击中用户心理 一篇长文教你学会推荐系统的矩阵分解 基于深度学习的广告CTR预估算法 距离玩转企业级DCN(Deep & Cross Network)模型，你只差一步 开源系统 基于用户行为 easyrec var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/tensorflow/dataset.html":{"url":"data-process/tensorflow/dataset.html","title":"dataset","keywords":"","body":"dataset https://zhuanlan.zhihu.com/p/37106443 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/tensorflow/distribute.html":{"url":"data-process/tensorflow/distribute.html","title":"分布式训练","keywords":"","body":"分布式训练 https://www.jianshu.com/p/fdb93e44a8cc var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/tensorflow/optimizer -compare.html":{"url":"data-process/tensorflow/optimizer -compare.html","title":"Optimizer Compare","keywords":"","body":"Optimizer Compare var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/tensorflow/saved-model.html":{"url":"data-process/tensorflow/saved-model.html","title":"saved_model","keywords":"","body":"saved_model 使用 saved_model_cli 检查及执行 saved_model CLI to inspect and execute SavedModel 查看 saved_model 详情 # 查看有哪些 tag-sets saved_model_cli show --dir /tmp/saved_model_dir # 查看 tag-set 里有哪些 SignatureDef saved_model_cli show --dir /tmp/saved_model_dir --tag_set serve # 查看一个具体的 signature_def 详情 saved_model_cli show \\ --dir /tmp/saved_model_dir --tag_set serve \\ --signature_def serving_default # 查看所有信息 saved_model_cli show --dir /tmp/saved_model_dir --all 执行 saved_model saved_model#run_command ### 把 checkpoint 转成 saved_model [Serving Inception Model with TensorFlow Serving and Kubernetes] 可以用链接中写的脚本， 或者直接 docker run ```bash docker run --rm -it --network=host -v D:\\models\\inception-v3\\:/models/inception registry.docker-cn.com/tensorflow/serving:nightly-devel bash # in docker container bazel build -c opt \\ tensorflow_serving/example:inception_saved_model bazel-bin/tensorflow_serving/example/inception_saved_model \\ --checkpoint_dir=inception-v3 --output_dir=models/inception ``` var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/DEFAULT.html":{"url":"data-process/top-10-data-mining-algorithm/DEFAULT.html","title":"十大数据挖掘数法","keywords":"","body":"十大数据挖掘数法 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/math/least-squares.html":{"url":"data-process/top-10-data-mining-algorithm/math/least-squares.html","title":"最小二乘法","keywords":"","body":"最小二乘法 xxx var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/AdaBoost.html":{"url":"data-process/top-10-data-mining-algorithm/AdaBoost.html","title":"AdaBoost.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/Apriori.html":{"url":"data-process/top-10-data-mining-algorithm/Apriori.html","title":"Apriori.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/C4.5.html":{"url":"data-process/top-10-data-mining-algorithm/C4.5.html","title":"数据挖掘十大经典算法之 C4.5","keywords":"","body":"数据挖掘十大经典算法之 C4.5 ID3 算法 ID3算法，即Iterative Dichotomiser 3，迭代二叉树3代 信息熵(Entropy) 信息熵(Entropy) 表示信息的混乱程度，变量的不确定性越大， 则熵值越大， 一个训练集的信息熵可以表示为： \r Entropy(S) = - \\sum _{i=1}^m P(u_i)\\log_2P(u_i) \r 其中 $P(u_i) = \\frac{|u_i|}{|S|}$ 表示为样本类别 i 在集合中出现的概率 信息增益(Information gain) 信息增益指的是划分前后熵的变化，可以用下面的公式表示： \r infoGain(S,A) = Entropy(S) - \\sum_{V\\in Value(A)}\\frac{S_V}{S}Entropy(S_V) \r 其中， A 表示样本的属性， $Value(A)$ 表示属性 A 的所有取值集合， V 是属性 A 的取值之一， $S_V$ 是 S 中 A 的值为 V 的样例集合。 ID3 算法便是每次从剩余属性集合中找出一个属性， 通过这个属性来划分集合使得信息熵增益最大。 C4.5 算法 信息增益率 \r infoGainRatio = \\frac{infoGain(V)}{H(V)}\r \r H(V)= -\\sum_j{p(v_j)\\log_2 p(v_j)}\r CART 使用 Gini 基数来评估样本分布 \r Gini(S) = 1 - \\sum_{i=1}^m {P_i^2}\r \r GiniGain(S,A) = Gini(S) - \\sum_{V\\in Value(A)} \\frac{S_V}{S} Gini(S_V) \r var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/CART.html":{"url":"data-process/top-10-data-mining-algorithm/CART.html","title":"CART.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/EM.html":{"url":"data-process/top-10-data-mining-algorithm/EM.html","title":"EM.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/k-means.html":{"url":"data-process/top-10-data-mining-algorithm/k-means.html","title":"k-means.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/kNN.html":{"url":"data-process/top-10-data-mining-algorithm/kNN.html","title":"kNN.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/Naive-Baye.html":{"url":"data-process/top-10-data-mining-algorithm/Naive-Baye.html","title":"Naive-Baye.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/PageRank.html":{"url":"data-process/top-10-data-mining-algorithm/PageRank.html","title":"PageRank.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/Svm.html":{"url":"data-process/top-10-data-mining-algorithm/Svm.html","title":"Svm.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/top-10-data-mining-algorithm/top-10-data-mining-algorithm.html":{"url":"data-process/top-10-data-mining-algorithm/top-10-data-mining-algorithm.html","title":"top-10-data-mining-algorithm.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/model-evaluation.html":{"url":"data-process/model-evaluation.html","title":"机器学习模型评估","keywords":"","body":"机器学习模型评估 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-process/serving.html":{"url":"data-process/serving.html","title":"Tensorflow Model Serving","keywords":"","body":"Tensorflow Model Serving serve multiple model var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-warehouse/user-portrait.html":{"url":"data-warehouse/user-portrait.html","title":"user-portrait.md","keywords":"","body":"https://blog.csdn.net/R3eE9y2OeFcU40/article/details/82880218 https://www.jianshu.com/p/4e36a03cf2dc https://zhuanlan.zhihu.com/p/52683548 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"data-warehouse/wen-zhang-zhai-xuan.html":{"url":"data-warehouse/wen-zhang-zhai-xuan.html","title":"Articles","keywords":"","body":"Articles 一文看懂什么是事实表， 维度表， 什么是星型模型， 雪花模型， 星座模型 三个例子，让你看懂数据仓库多维数据模型的设计 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/docker/an-zhuang-hou-mian-sudo.html":{"url":"devops/docker/an-zhuang-hou-mian-sudo.html","title":"安装后免 sudo ","keywords":"","body":"安装后免 sudo sudo usermod -aG docker $USER var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/gong-ju/install-latest-git-version-on-centos.html":{"url":"devops/gong-ju/install-latest-git-version-on-centos.html","title":"install-latest-git-version-on-centos.md","keywords":"","body":"https://stackoverflow.com/questions/21820715/how-to-install-latest-version-of-git-on-centos-6-x-7-x var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/iptables.html":{"url":"devops/linux/iptables.html","title":"iptables","keywords":"","body":"iptables 端口转发 # 把发往 192.168.2.61 3001 端口的流量转发到 192.168.2.70:3306 如转发所有 ip -d 192.168.2.61 可不写 iptables -t nat -A PREROUTING -d 192.168.2.61 -p tcp --dport 3001 -j DNAT --to-destination 192.168.2.70:3306 # 把 192.168.2.70 回来的 tcp ip 改写成 192.168.2.70 iptables -t nat -A POSTROUTING -d 192.168.2.70 -p tcp --dport 3306 -j SNAT --to 192.168.2.61 删除规则 iptables -t nat -D PREROUTING 1 保存 iptables iptables-save > /etc/iptables.rules 恢复 iptables iptables-restore var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/network.html":{"url":"devops/linux/network.html","title":"network","keywords":"","body":"network 支持非root 用户使用 80 端口 setcap cap_net_bind_service=+ep /usr/local/bin/node var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/performance-diagnosis.html":{"url":"devops/linux/performance-diagnosis.html","title":"performance-diagnosis.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/shell-syntax.html":{"url":"devops/linux/shell-syntax.html","title":"Shell Syntax","keywords":"","body":"Shell Syntax Var Default value Foo=${Foo:-'Default'} var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/ssh-with-kerberos.html":{"url":"devops/linux/ssh-with-kerberos.html","title":"SSH with Kerbors5 on Ubuntu","keywords":"","body":"SSH with Kerbors5 on Ubuntu kerberos server and admin server install sudo apt install krb5-kdc krb5-admin-server key config in /etc/krb5.conf [libdefaults] default_realm = EXAMPLE.COM [realms] EXAMPLE.COM = { kdc = kdc.example.com admin_server = kbr-admin.example.com } #init realm # may be take long time to wait after the notice `Loading random data` krb5_newrealm 添加用户 kadmin.local addprinc username # 给机器 s1 添加 Princple 及 keytab ， 并将该 keytab 复制到 s1 机器 /etc 目录下， sshd 会用到 krb5.keytab, 确定主机名一致 kadmin.local -q \"addprinc -randkey host/s1.dev.example.com\" kadmin.local -q \" ktadd -k /etc/krb5.keytab host/s1.dev.example.com\" ssh server 保证跟 kdc 一样的 krb5.conf [libdefaults] default_realm = EXAMPLE.COM [realms] EXAMPLE.COM = { kdc = kdc.example.com admin_server = kbr-admin.example.com } 修改 /etc/ssh/sshd_config KerberosAuthentication yes KerberosTicketCleanup yes GSSAPIAuthentication yes GSSAPICleanupCredentials yes 在 $HOME/.k5login 中列出允许登录的 princ user1@EXAMPLE.COM ssh client install kerberos client apt install krb5-user # 初始化 ticket kinit user xshell 通过 kerberos 登录 安装 MIT Kerberos for Windows 4.1 64-bit MSI Installer kfw-4.1-amd64.msi, 10812k. 32-bit MSI Installer kfw-4.1-i386.msi, 5836k. 使用上面的 krb5.conf 覆盖到 C:\\ProgramData\\MIT\\Kerberos5\\krb5.ini, 或通过 KRB5_CONFIG 环境变量指定 krb5.ini 位置 参考文章 https://help.ubuntu.com/lts/serverguide/kerberos.html.en#kerberos-server http://www.visolve.com/ssh.php#Kerberos_Authentication https://docs.oracle.com/cd/E19253-01/819-7061/6n91j2vds/index.html var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux/wget.html":{"url":"devops/linux/wget.html","title":"wget ","keywords":"","body":"wget 下载网站镜像 wget -c -r -np -k -L -p www.xxx.org/pub/path/ -c 断点续传 -r 递归下载，下载指定网页某一目录下（包括子目录）的所有文件 -nd 递归下载时不创建一层一层的目录，把所有的文件下载到当前目录 -np 递归下载时不搜索上层目录，如wget -c -r www.xxx.org/pub/path/ ， 没有加参数-np，就会同时下载path的上一级目录pub下的其它文件 -k 将绝对链接转为相对链接，下载整个站点后脱机浏览网页，最好加上这个参数 -L 递归时不进入其它主机 -p 下载网页所需的所有文件，如图片等 xxx var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/network/install-merlin-on-asus-router.html":{"url":"devops/network/install-merlin-on-asus-router.html","title":"install-merlin-on-asus-router.md","keywords":"","body":"零基础AC68U刷梅林，超详细每步都有图 直接用 tftp 刷入， 不需要用华硕提供的救援模式软件 搜索： 华硕路由器(RT-AC66U)进入救援模式从小宝梅林固件等第三方固件刷回官方原厂固件的方法 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/docker.html":{"url":"devops/docker.html","title":"docker.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/linux.html":{"url":"devops/linux.html","title":"Performance diagnosis","keywords":"","body":"Performance diagnosis 查看进程内存占用 方法一 pmag pmag -x 1 1: java -jar /xxx/x.jar Address Kbytes RSS Dirty Mode Mapping 0000000000400000 6736 2692 0 r-x-- php-cgi 0000000000c93000 264 196 120 rw--- php-cgi 0000000000cd5000 60 48 48 rw--- [ anon ] . . . 00007fd6226bc000 4 4 4 rw--- ld-2.12.so 00007fd6226bd000 4 4 4 rw--- [ anon ] 00007fff84b02000 96 96 96 rw--- [ stack ] 00007fff84bff000 4 4 0 r-x-- [ anon ] ffffffffff600000 4 0 0 r-x-- [ anon ] ---------------- ------ ------ ------ total kB 438284 113612 107960 关键信息点 进程ID 启动命令「java -jar /xxx/x.jar」 RSS :占用的物理内存 113612KB 方案二 cat /proc/1/status var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"devops/rabbitmq.html":{"url":"devops/rabbitmq.html","title":"RabbitMQ","keywords":"","body":"RabbitMQ RabbitMQ及Erlang内存使用分析 rabbitmq 3.6.2 内存持续增长问题 rabbitmqctl eval 'supervisor2:terminate_child(rabbit_mgmt_sup_sup, rabbit_mgmt_sup),rabbit_mgmt_sup_sup:start_child().' var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/dubbo/":{"url":"langue-and-frameworks/dubbo/","title":"README.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/spring/spring-mvc/zi-ding-yi-can-shu-jie-xi.html":{"url":"langue-and-frameworks/java/spring/spring-mvc/zi-ding-yi-can-shu-jie-xi.html","title":"扩展点","keywords":"","body":"扩展点 自定义异常处理 org.springframework.web.servlet.HandlerExceptionResolver 自定义 http 参数解析与封装 org.springframework.web.method.support.HandlerMethodArgumentResolver var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/spring/chang-yong-kuo-zhan.html":{"url":"langue-and-frameworks/java/spring/chang-yong-kuo-zhan.html","title":"常用扩展","keywords":"","body":"常用扩展 自定义 Resource 及加载 org.springframework.web.servlet.resource.ResourceResolver var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/spring/spring-mvc.html":{"url":"langue-and-frameworks/java/spring/spring-mvc.html","title":"Srping MVC","keywords":"","body":"Srping MVC var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/how-to-use-g1-garbage-collector.html":{"url":"langue-and-frameworks/java/how-to-use-g1-garbage-collector.html","title":"How to use G1 garbage collector","keywords":"","body":"How to use G1 garbage collector 参数 描述 -XX:+UseG1GC 开启G1 -XX:MaxGCPauseMillis=n 设置GC暂停的最大时间,这只是目标,尽量达到,默认值是 200 毫秒,过小影响吞吐量 -XX:InitiatingHeapOccupancyPercent=n 整个堆(而不是某个年代)使用量达到此值,便会触发并发GC周期.值为0则是连续触发,默认值为45 -XX:NewRatio=n 老年代与新生代的比值,默认值为2 -XX:SurvivorRatio=n 伊甸园代与生存代的比率,默认值为8 -XX:MaxTenuringThreshold=n 生存代存活的最大门限,默认值为15 -XX:ParallelGCThreads=n 设置垃圾回收器并行阶段的线程数,默认值与JVM运行的平台有关,将 n 的值设置为逻辑处理器的数量 -XX:ConcGCThreads=n 设置并发垃圾回收器使用的线程数,默认值与JVM运行的平台有关 -XX:G1ReservePercent=n 设置剩余的内存量,减少跃迁失败的可能,默认值为10 -XX:G1HeapRegionSize=n 设置G1平分java堆而产生区域的大小,默认值可以提供最大的工效性.最小值为1M,最大为32M,最多划分1024个,建议使用默认值 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/print-gc-log.html":{"url":"langue-and-frameworks/java/print-gc-log.html","title":"How to Print GC log","keywords":"","body":"How to Print GC log -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 当使用G1 回收器时可使用参数 -verbosegc (等价于 -XX:+PrintGC) 设置日志级别为 好fine. [GC pause (G1 Humongous Allocation) (young) (initial-mark) 24M- >21M(64M), 0.2349730 secs] [GC pause (G1 Evacuation Pause) (mixed) 66M->21M(236M), 0.1625268 secs] var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/java/spring.html":{"url":"langue-and-frameworks/java/spring.html","title":"spring.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/nodejs/pei-zhiregistry-jing-xiang-huo-dai-li.html":{"url":"langue-and-frameworks/nodejs/pei-zhiregistry-jing-xiang-huo-dai-li.html","title":"npm 配置 registry 镜像或代理","keywords":"","body":"npm 配置 registry 镜像或代理 配置 registry 镜像 命令行配置npm config set registry http://registry.cnpmjs.org npm info underscore （如果上面配置正确这个命令会有字符串response） 命令行指定npm --registry http://registry.cnpmjs.org info underscore 编辑 ~/.npmrc registry = http://registry.cnpmjs.org 配置代理 npm config set proxy http://server:port npm config set https-proxy http://server:port 或 npm config set proxy http://username:password@server:port npm confit set https-proxy http://username:password@server:port 使用 nrm 快速切换npm源 安装 sudo npm install -g nrm 使用 列出可用的源： ~ nrm ls npm ---- https://registry.npmjs.org/ cnpm --- http://r.cnpmjs.org/ taobao - http://registry.npm.taobao.org/ eu ----- http://registry.npmjs.eu/ au ----- http://registry.npmjs.org.au/ sl ----- http://npm.strongloop.com/ nj ----- https://registry.nodejitsu.com/ pt ----- http://registry.npmjs.pt/ var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/spring/maintenance-instructions.html":{"url":"langue-and-frameworks/spring/maintenance-instructions.html","title":"Hive Maintenance Instructions","keywords":"","body":"Hive Maintenance Instructions MSCK REPAIR TABLE table_name; 可用于 hdfs 文件发生变化后修复表 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/spring/":{"url":"langue-and-frameworks/spring/","title":"配置项","keywords":"","body":"配置项 mapreduce.map.memory.mb 用于处理 Map 内存超出问题 mapreduce.reduce.memory.mb 用于处理 reduce 内存超出问题 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"langue-and-frameworks/spring/xxx.html":{"url":"langue-and-frameworks/spring/xxx.html","title":"xxx.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mongodb/":{"url":"storage-system/mongodb/","title":"README.md","keywords":"","body":"var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mysql/operation/import-export.html":{"url":"storage-system/mysql/operation/import-export.html","title":"数据导入导出","keywords":"","body":"数据导入导出 ## 导出 mysqldump -h ${host} -u ${user} -p ${db} ${table} [--where='1=1'] > file.sql ## 导入 mysql -u root -p ${db} var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mysql/install-by-yum.html":{"url":"storage-system/mysql/install-by-yum.html","title":"Install Mysql Server by Yum","keywords":"","body":"Install Mysql Server by Yum Edit /etc/yum.repos.d/mysql-community.repo [mysql57-community] name=MySQL 5.7 Community Server baseurl=http://repo.mysql.com/yum/mysql-5.7-community/el/6/$basearch/ enabled=1 gpgcheck=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-mysql Install and Start yum install mysql-community-server service mysqld start 启动后到 /var/log/mysqld.log 找初始密码 Securing the MySQL Installation mysql_secure_installation var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mysql/mysql-diff.html":{"url":"storage-system/mysql/mysql-diff.html","title":"mysql-diff.md","keywords":"","body":"https://dev.mysql.com/doc/mysql-utilities/1.5/en/mysqldiff.html var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mysql/settings.html":{"url":"storage-system/mysql/settings.html","title":"Mysql Settings","keywords":"","body":"Mysql Settings 设置sql 模式 SHOW VARIABLES LIKE 'sql_mode'; set global sql_mode= 'ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'; set global sql_mode= 'ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION' var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"storage-system/mysql/user-manager.html":{"url":"storage-system/mysql/user-manager.html","title":"User Management","keywords":"","body":"User Management add root user CREATE USER 'root'@'10.1.%' IDENTIFIED BY 'password'; grant all privileges on *.* to 'root'@'10.1.%' with grant option; FLUSH PRIVILEGES; -- 表级授权 grant select on dap.t_kmap_nutrition_billfare_info to 'food-ro'@'10.1.%'; var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"},"tools/intellij-idea.html":{"url":"tools/intellij-idea.html","title":"Intellij IDEA","keywords":"","body":"Intellij IDEA 指定 idea 运行时使用的 jdk -10-8-6-4-202468100.80.60.40.20.0-0.2-0.4-0.6-0.8 var targetUl = document.getElementsByClassName('page-inner')[0].getElementsByTagName('ul')[0];if(targetUl.getElementsByTagName('a').length>0){targetUl.className='toc';}"}}